
% JuliaCon proceedings template
\documentclass{juliacon}
\setcounter{page}{1}
\usepackage{xspace}
\newcommand{\utd}{\texttt{UnitTestDesign}\xspace}

\begin{document}

\input{header}

\maketitle

\begin{abstract}

The \utd package generates all-pairs test cases for unit tests. It implements a greedy algorithm that minimizes the number of test cases that give $n$-way combinatorial coverage. This library is meant to be convenient to use within Julia's unit testing framework, so you can specify forbidden combinations of arguments, to match the behavior of the function under test.

\end{abstract}

\section{Introduction}

We test code in order to address risk of failure, and we judge risk of failure by the likelihood code will fail, how difficult it is to mitigate a failure, and how important it is to have a correct result. However, whether the result of a calculation matters less than what's in the environment~\cite{Wiklund2017-ms}.
\vskip 6pt
Our first unit tests for code often derive from mimicking how a user might write client code. These happy-path tests are narrative. We may augment them with our best guesses for corner cases of a function under test. There are times we want more of a guarantee that a function doesn't have faults. This may be that we recognize the function is complex, or that it's central to a sensitive calculation. We may test a part of a code base well because we want reduce the logical complexity that comes from uncertainty~\cite{Sha2001-ie}.

\vskip 6pt
There is a world of testing tools to help find faults in risky code. Some measure the quality of existing tests. Others select tests to run depending on test coverage or recent code modifications. Let's focus on the first antidote to the biases inherent in narrative testing, those that generate test cases.

\vskip 6pt
\begin{lstlisting}[language=Julia]
using UnitTestDesign

@enum Strategy naive keyfitz greville monotonic
arg1 = [1, 4, 8]
arg2 = [:a, :b, :c, :d]
arg3 = [naive, keyfitz, greville, monotonic]
arg4 = ["none", "RK4", "quadrature", "QUADPACK"]
arg5 = [0.0, 9.3, -1.2, Inf]

test_cases = all_pairs(arg1, arg2, arg3, arg4, arg5)
for test_case in test_cases:
    result = samplefunction(test_case...)
    @assert known_invariant(result, test_case)
end
\end{lstlisting}

Here, the \verb|all_pairs| function uses the sample argument values to create a set of test cases. The question is which set of cases it creates and when they are appropriate.

\vskip 6pt
There are a few common strategies for automated test case generation. A randomized approach chooses argument values from the space of allowed arguments, usually with some bias towards choosing possible corner cases~\cite{Lampropoulos2020-sd,Arcuri2012-az}. Some tools introduce more structure to choosing random arguments. The \texttt{QuickCheck} and \texttt{Hypothesis} packages use customized generators to create streams of test cases~\cite{loscher2018automating}. Concolic testing records the execution of the function under test in order to generate subsequent test cases that are likely to increase line coverage~\cite{King1976-jt,Wang2018-xh,vira2019}. There are also more mathematical approaches, such as Sobol sequences and orthogonal arrays~\cite{He2013-th}, which have particular diagnostic uses.


\section{Statement of Need}

Most of the techniques for test case generation focus more on making a smart set of tests than they focus on making few tests that are of high quality. Combinatorial testing generates fewer tests that are designed to give good branch coverage~\cite{Nie2011-yl,Grindal2005-su,Kuhn2010-ak}.

\vskip 6pt
There are two cases where this is crucial. The first is that you have to test a slow function, such as a large, Monte Carlo inference. If the function under test runs for hours or days, then selective testing can respect resource limitations.

\vskip 6pt
The other case is when there is a large test space. We've been talking about a function under test, but this is a stand-in for any test we can parametrize. Let's say we've written an application, that we've unit tested its important supporting functions, and we want to write user-level tests that look for problems from interactions among command-line arguments and choices in parameter files. This can be equivalent to testing a function with twenty or more arguments. Twenty arguments, with four possible values each, would lead to over $10^{12}$ ways to call this function, so how can we feel our tests begin to cover the problem?

\vskip 6pt
The combinatorial testing in \utd takes the time to optimize the choice of test cases so that they can test the code well. The theory of the game is that each if--then in the code will branch depending on combinations of argument values, so combinatorial testing ensures all combinations of $n$ arguments are tried in at least one test case. Here, $n=2$ for two-way testing, also known as all-pairs testing, but studies have shown that wayness up to $n=6$ can be useful.


\vskip 6pt
While combinatorial testing is a useful tool in some circumstances, there are other tools that generate combinatorial tests. X makes them with a domain-specific language. This company's tool does it.

\vskip 6pt
These companies keep disappearing. The quality of tools is supect (by the authors sometimes). It comes down to an author using the tools available within a framework.

\vskip 6pt
That means the tool has to be usable, too. What happens if we create an all-pairs set of unit tests, but some of them don't work for your function because the combination of arguments doesn't make sense? You have to throw out those tests, but you've lost the very guarantee that this testing method made.
So it has to have a) forbid combinations of argument values, b) n-way testing c) multi-way testing d) start with certain cases~\cite{Czerwonka2006-hm}.


\section{How to Use Combinatorial Testing}

\subsection{Outline of the method}

While you may already know how to use test case generation from the snippet in the introduction, it can help to think of automated test case generation as a set of steps, each of which has choices to make.
\begin{enumerate}
   \item Identify the system under test. It may be a function, but it could correspond to an \textsc{api} or an application.
   \item Decide how a single test case, consisting of argument values, should correspond to a unit test.
   \item Evaluate the risk associated with different arguments.
   \item Pick a test generation strategy.
   \item Choose a method for checking the results of each test.
\end{enumerate}
We'll explore these steps in this section.

\subsection{Identify the system under test}

We have assumed, thus far, that we are unit testing a function. In that case, the function has arguments and those arguments can take on particular values. Our test cases will correspond exactly to an invocation of the function. However, this need not be the case. The tool can be used more generally. Combinatorial testing is used outside Julia. It would apply, for instance, to testing compatibility of hardware subsystems.

A common case in Julia is testing a scientific application at the user level. Here, we can consider the user's choices of command-line arguments and parameters in files. We could even generate classes of data according to keys that we assign to arguments in \utd.

\subsection{Decide argument values}

Some arguments to a function can take on only a few allowed values, such as \verb|true| or \verb|false|, but most arguments are integers, floating-point, or more complicated data structures. The theory of covering arrays presumes any argument will be assigned only one of a few values, so we need to make choices.

\vskip 6pt
We're looking for faults in the code that lead to failures of unit tests. These faults will be visible, or invisible, for different choices of argument values. If a given function fails for \verb|arg3<2.7|, then it will fail for \verb|arg3=2| as well as \verb|arg3=2.5|, so we consider those equivalent with respect to finding faults. Our goal, as authors of unit tests, is to select argument values that represent equivalence classes for the function under test. There isn't a lot of guidance on how to choose these values. You can guess them by looking at the code or by looking at a specification for the function.

\vskip 6pt
One way to defend against our uncertainty about equivalence classes is to combine combinatorial test generation with random testing. For each argument, partition the space of values into equivalence classes. Then generate test cases, but use the values in those test cases to select randomly from the partitions. This is one way to introduce bias, and control, into random test selection. It points to treating test case generation as a continuum of techniques.

\vskip 6pt
The argument values in test cases could also represent, for instance, the number of times to call a function on an \textsc{api}. The argument values could represent choices about what arguments to pass to each function in a series of function calls.

\vskip 6pt
There are cases where a function has one argument that is a flag and, when it's chosen, other argument values don't make sense to choose. We want to forbid these from the possible input choices. For instance, if \verb|arg1| can be \verb|true| or \verb|false|, and \verb|arg2| can be \verb|default|, \verb|high|, or \verb|low|, but we want to exclude the combination of \verb|false| and \verb|high| or \verb|low|, then the possible outcomes are \verb|true|-\verb|default|, \verb|true|-\verb|high|, \verb|true|-\verb|low|, \verb|false|-\verb|default|. We can either teach the test generator to forbid two combinations, or we can modify the input values, so that \verb|arg12| represents the four possible cases for \verb|arg1| and \verb|arg2| together.

Forbidden tuples~\cite{Petke2015-ex} for combinatorial testing is a method. Among others~\cite{Grindal2006-vy}.


\subsection{Evaluate risk}

The combinatorial testing approach gives you a lever with which to address perceived risk in code. This can range from pairwise testing, which is two-way coverage, to six-way coverage, beyond which studies show little benefit~\cite{Petke2015-ex}. corresponds to if--then.

depends on resources.

vary with sets of arguments.
Mixed-level covering arrays don't have well-understood bounds~\cite{Cohen2003-pg}.

Can after-the-fact measure differences in fault-finding.


\subsection{Pick a test generation strategy}

full factorial
excursion
n-way coverage
 - by greedy
 - by greedy, non-deterministic


\subsection{Evaluate results}

tend to use traits.

\vskip 6pt
Do the results represent exact values? For mathematical code, we often don't have exact values. You may check the results with
\begin{itemize}
   \item A different version of the same function, some kind of parallel implementation. Could be a more naive algorithm or one that uses a different mathematical representation of the same calculation.
   \item A trait of the result, given the inputs. For instance, it may be that a second execution of the function under test, with the same arguments, modified to have \verb|f(2*a, b/2)=f(a, b)| should give the same result. This is a symmetry test~\cite{Segura2016-qh}.
\end{itemize}

\section{Optimization of Test Cases with Practical Constraints}
Also \textsc{aetg}~\cite{Cohen1997-lb}.

\vskip 6pt
So what we do here: Help anybody do a good job on this algorithm, thanks to some work we did.
Clarify the greedy algorithm~\cite{Lei2008-xt}. Show the algorithm with its complexity.
Out of scope: advanced data structures~\cite{Segall2011-jv}, other optimization methods.

\vskip 6pt
An argument is. A value is.

\vskip 6pt
The class of greedy algorithms:
The end result will be a list of test cases that have A columns, where A is the number of arguments.
\begin{enumerate}
   \item Generate a list of all combinations of argument values. For instance, one entry would be that the first value of the second argument must appear with the fourth value of the third argument.
   \item For an n-way coverage algorithm, initialize the first n columns of the test cases with a full factorial set of values.
   \item For each argument from n+1 to A.
   \begin{enumerate}
      \item Grow the test cases wider by finding all cases that match the next empty argument.
      \item Grow the test cases taller by adding a row for all cases that couldn't be matched.
   \end{enumerate}
   \item Fill in any missing values in test cases.
\end{enumerate}

Higher arity for greedy algorithms is a relatively recent development~\cite{Richard_Kuhn2008-ut}.

\vskip 6pt
\begin{lstlisting}[language=Julia]
function ipog(arity, n_way)
    nonincreasing = sortperm(arity, rev = true)
    arity = arity[nonincreasing]

    param_cnt = length(arity)
    # 2D array of argument values.
    test_set = all_combinations(
            arity[1:n_way], n_way)

    for param_idx in (n_way + 1):param_cnt
        taller = zeros(eltype(arity), param_idx,
                size(test_set, 2))
        taller[1:(param_idx - 1), :] .= test_set
        allc = one_parameter_combinations_matrix(
                arity[1:param_idx], n_way)
        choose_last_parameter!(taller, allc)
        test_set = insert_tuple_into_tests(
                taller, allc)
    end

    fill_remaining_missing_values!(test_set, arity)
    test_set[sortperm(nonincreasing), :]
end
\end{lstlisting}

\vskip 6pt
If we pick one entry from the proposal and one entry from the test cases, there are five ways they can compare.
\begin{center}
\begin{tabular}{rll}
name & test cases & tuples \\ \hline
ignores & * & * \\
skips & $a$ & * \\
misses & * & $b$ \\
matches & $a$ & $b=a$ \\
mismatches & $a$ & $b\ne a$
\end{tabular}
\end{center}
Only the last comparison is clearly a mismatch.

\vskip 6pt
Let's take two strings of argument values. Each pair of arguments will compare with one of the five ways. If any mismatch, there is a mismatch of the two strings. That leaves several different ways we can call a string matching. We can think of the string as scoring either zero or more than zero ignores, zero or more than zero skips, and so-on. That means there are $2^4-1 = 15$ ways for two strings to match, because nonzero strings will match at least one of the four ways. Which of the 15 ways do we count as a match?


\begin{enumerate}
   \item No mismatches.
   \item Some matches and no mismatches.
   \item Only matches, skips, or ignores. No misses or mismatches.
\end{enumerate}

\vskip 6pt
1 and 2 are equivalent when the tuples all have a value in the current column being filled.
For filling the last tests, using no-mismatches works well.

\section{Comparison of Approaches}
Classic criticism that coverage shouldn't be your goal~\cite{Inozemtseva2014-gz}. Counterpoint that coverage can be more meaningful for less-used paths in the code~\cite{Cai2005-ex}.

\vskip 6pt
to random
to generative

\vskip 6pt
From a user perspective, it's about how easy it is to use one method or another. For instance, QuickCheck does more than generate tests. It reduces the cycle of debugging by finding the simplest test that fails.

\vskip 6pt
to concolic~\cite{King1976-jt,Wang2018-xh}.


\section{Conclusion}
Residual logical complexity.

Combinatorial testing can benefit from simulated annealing~\cite{Petke2015-ex}.


\vskip 6pt
The person writing the unit tests has to pick the input values. In the same way that you would build up a decision table by selecting values from equivalence classes, you select values here. It can be an advantage in that you control the list and can make it more focused where you expect risk. It can be a disadvantage because your choice could exclude a value that random testing or concolic testing might discover exposes a fault.

Another greedy algorithm~\cite{Calvagna2012-ic,Koc2018-vs}.

This would work well with automated test selection if the the Julia testing framework defined tests such that they could be selected, as is done in X package.


\vskip 6pt
Unit test is a second telling of a function. A parable.

\input{bib.tex}

\end{document}

% Inspired by the International Journal of Computer Applications template
