<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Usage · UnitTestDesign.jl</title><link rel="canonical" href="https://adolgert.github.io/UnitTestDesign.jl/man/usage/"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit">UnitTestDesign.jl</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../features/">Features</a></li><li class="is-active"><a class="tocitem" href>Usage</a><ul class="internal"><li><a class="tocitem" href="#Test-generation"><span>Test generation</span></a></li><li><a class="tocitem" href="#Partition-testing"><span>Partition testing</span></a></li><li><a class="tocitem" href="#Random-testing"><span>Random testing</span></a></li><li><a class="tocitem" href="#Test-selection"><span>Test selection</span></a></li></ul></li><li><a class="tocitem" href="../../reference/">Reference</a></li><li><a class="tocitem" href="../../contributing/">Contributing</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Usage</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Usage</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/adolgert/UnitTestDesign.jl/blob/master/docs/src/man/usage.md#L" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Usage-in-context"><a class="docs-heading-anchor" href="#Usage-in-context">Usage in context</a><a id="Usage-in-context-1"></a><a class="docs-heading-anchor-permalink" href="#Usage-in-context" title="Permalink"></a></h1><h2 id="Test-generation"><a class="docs-heading-anchor" href="#Test-generation">Test generation</a><a id="Test-generation-1"></a><a class="docs-heading-anchor-permalink" href="#Test-generation" title="Permalink"></a></h2><p>We use test case generation tools, like this library, when tests take too long to run. It can be worthwhile to generate smaller test suites by running slower test case generators (such as <a href="../../reference/#UnitTestDesign.GND"><code>GND</code></a>). The development process is:</p><ol><li>Generate test cases.</li><li>Save test cases to a file, a testing artifact, or copied to the unit test as data.</li><li>Load the test cases for unit testing.</li></ol><h2 id="Partition-testing"><a class="docs-heading-anchor" href="#Partition-testing">Partition testing</a><a id="Partition-testing-1"></a><a class="docs-heading-anchor-permalink" href="#Partition-testing" title="Permalink"></a></h2><p>We&#39;ve pretended, so far, that every function takes parameters that are selected from discrete, finite choices. Functions arguments can be floating-point values, integers from infinite sets, vectors of values, or trees of trees of values. <em>Partition testing</em> whittles down the nearly-infinite possible values into subsets which are likely to find the same faults in the function-under-test.</p><p>If a function integrates a value from <code>a</code> to <code>b</code>, then we could partition the possible <code>a</code> and <code>b</code> test values into those that are both negative, both positive, one negative and one positive. We could make a separate case for when they are very nearly equal or equal. That would make five partitions for these two variables. For each of those five partitions, we guess that it would be enough to choose some example value for testing.</p><pre><code class="language-julia">test_set = all_pairs(
    [:neg, :pos, :split, :near, :far], [0.1, 0.01, 0.001], [:RungeKutta, :Midpoint]
    )
ab = Dict(:neg =&gt; (-3, -2), :pos =&gt; (2, 3), :split =&gt; (-2, 3),
    :near =&gt; (4.999, 5), :far =&gt; (0, 1e7)
)
for test_case in test_set
    a, b = ab[test_case[1]]
    result = custom_integrate(a, b, test_case[2:end]...)
    @test result == compare_with_symbolic_integration(a, b)
end</code></pre><h2 id="Random-testing"><a class="docs-heading-anchor" href="#Random-testing">Random testing</a><a id="Random-testing-1"></a><a class="docs-heading-anchor-permalink" href="#Random-testing" title="Permalink"></a></h2><p>Random testing side-steps the work, and fallibility, of partition testing by choosing input values randomly from their domain. If <code>a</code> can be any number between 0 and infinity, then let a random number generator pick it.</p><p>Random testing isn&#39;t blind to understanding what can go wrong in a function. It&#39;s no reason to forget to test edge cases. There is a procedure for constructing random tests that are biased towards finding edge cases.</p><ol><li>Identify the whole domain of each parameter and the set of parameters.</li><li>Assign weights to bias selection on that domain.</li><li>Sample from the parameters, given the weights.</li></ol><p>For instance, if a parameter were a string, you wouldn&#39;t generate from all random strings. You&#39;d sample first for a string length, with assurance that lengths 0 and 1 are included. Then you&#39;d draw values in the string.</p><p>The test case generation in this library can help bias random testing. For instance, you could assign one partition to edge cases near <code>a=0</code>, one partition to general cases for <code>0.1 &lt; a &lt; 1000</code>, and one partition to high cases, <code>1e7 &lt; a &lt; Inf</code>. Let the test case generator say which of the low, mid, or high cases to choose, and then randomly choose values within each test case.</p><h2 id="Test-selection"><a class="docs-heading-anchor" href="#Test-selection">Test selection</a><a id="Test-selection-1"></a><a class="docs-heading-anchor-permalink" href="#Test-selection" title="Permalink"></a></h2><p>We can generate a lot of tests. How do we know they are a good set of tests? I&#39;d like to make lots of tests and then keep only those that are sufficiently different from the others that they would find different failures. I don&#39;t see any of these tools currently in Julia.</p><p>One measure is line coverage of code. We use all-pairs in order to to generate tests that cover every path through the code. An if-then in the code makes its decision based on variables which, in some way, depend on input parameters. If we include every combination of parameters, we will tend to cover more of the decisions of the if-thens.</p><p>A better measure is mutation analysis. This technique introduces errors into the code, on the fly. Then it runs unit tests against that code in order to ask which unit tests find the same failures. If two unit tests consistently find the same failures, then delete one of them and keep the other.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../features/">« Features</a><a class="docs-footer-nextpage" href="../../reference/">Reference »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Tuesday 1 December 2020 19:11">Tuesday 1 December 2020</span>. Using Julia version 1.5.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
